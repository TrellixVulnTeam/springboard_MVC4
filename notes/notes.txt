Courses I said I took but did not:
CatBoost: the New Generation of Gradient Boosting (section 13.4)


4.1: 4,5,6,7
5.4: 2,3
5.6: 2,3,4
7.1: 2
7.2: 1,2,3,4
8.2: 3,4
8.3: 2
8.5: 2,3,4,5,6
9: Everything
11.2: 2,5
13.2: Everything
14.2: Everythng
16.1: 5






Regex Notes: https://speakerdeck.com/pycon2017/al-sweigart-yes-its-time-to-learn-regular-expressions
Scaling: https://pub.towardsai.net/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff
Missing Data: https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html
Imbalanced Data: https://www.springboard.com/workshops/ai-machine-learning-career-track/learn#/curriculum/17811/17813
LinkedIn Learning Class: https://www.linkedin.com/learning/python-for-data-science-essential-training-part-2/machine-learning-rocks?u=36492188
Lecture learning series: https://bloomberg.github.io/foml/#lectures


Linear Regression Assumptions
Linearity: The relationship between X and the mean of Y is linear.
Homoscedasticity: The variance of residual is the same for any value of X.
Independence: Observations are independent of each other.
Normality: For any fixed value of X, Y is normally distributed.


Classificaiton Descriptions:
https://medium.com/@vijaya.beeravalli/comparison-of-machine-learning-classification-models-for-credit-card-default-data-c3cf805c9a5a


Naive Bayes:
Classificaiton algorithm based on Bayes's Theorem which gives an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a feature in a class is unrelated to the presence of any other feature
Pros: Fast, small amount of data needed
Cons: bad estimator

K Nearest Neighbors:
Gives a classification based on classification of other neighbors 


Decision Tree:
Builds classification model in a tree structure. 
Categorical and numerical
Simplicity to understand and visualize
Quite unstable because a change in data can change the entire decision tree

Benefits:
Nonlinear
Can be graphically represented
Requires less data prep (no one hot encoding)
Good with missing features
you can also use the resutling tree space as features for other models (ie, the leaf space)

Assumptions:
Root node = entire training set
Predictive featuresmust be cateogrical or binned (if continuous)
Rows in the dataset have a recursive distribution based on the values of the attributes

Random Forrest:
Many decision trees, can give average of multiple trees.
More accurate than decision tree
Quite complex in implementation and slow in real time prediction
Bootstrapping is random sampling of data with replacement
Bagging (Bootrap Aggregation) is performing bootstrapping many times and building an estimator for each bootstrapped dataset
Bagging works well (averaging the estimators) because the data is selected randomly.  If there is a correlation in the data, then the average isn't as powerful
Bagging works well on high variance, low bias models (like trees.  Linear models for example are low variance, high bias)

Difference bn Random Forrest and Bagging is that a RF will reduce the amount of features it builds a tree on.  This occurs because bootstrapping from the same dataset isn't random.  By reducing the features for a tree, we create more randomness. 
RF is highly parralizeable 
Very little tuning required
handle missing data
it's a go to method. but it's slower than linear method.  faster than a big NN

Stacking:
Building an ensemble of many models. The ensemble uses learning to figure out how to combine the models together. 

ANN:
Poor inte

SVM:

Hyperparmeters to train:
C = penalty term that dertermines how closely the model fits to the training set. Basically a regularization parameter.  Large penalty for misclassifciation in training when C is big. This gives potential for overfit. 
As C goes to 0, it will give a large margin of error. 
Kernel to pick from is also a Hyperparmeter 
classifier that represents training data in poitns in space and then 

Uses a subset of data, which makes it memory efficient and is good with high dimensial space
does not give probability estimates

When to use SVM:
Binary target variable
Feature to row ratio is high (short and wide data)
Very complex relationships
lots of outliers

When not to use SVM:
feature to row ratio is very low (SVM is slow with lots of data)
If transparency is important
Looking for a quick benchmark model.  Slow in training and prediction

XGBoost:
eXtreme Gradient Boosted treest
each tree bosts attributes that led to misclassifications of previous tree. 
Very good performer, easy to use and fast. 
A good choice to start with this first. 
Good for handling missing values
You can plug in your own optimization function
Allow for early stopping
Incremental training and parallel processing


Booster: Tree for classification, gbliner for linear
Objective: multi: softmax, multi: softprob
Eta: learning rate - adjust weights on each step (think of this as learning rate). lowering to .2 or lower will give better results
Max_depth (too large and overfit, too small and bad performance)
Min_child_weight (too high and you underfit)

Deep Learning:
Regularization: Most common is called dropout.  Randomly set some activations to 0. 
Another regularization technique is early stopping.  You want to stop when performance on testing data diverges from performance on training data. 

Adaptive Learning: Change the learning rate based on a function.  

TF 2:
Sequential API used for building stacks
Functional API used for buildig DAGs


Architecture types and when to use:
Images (Start with LeNet-like, then later on use ResNet)
Sequences (Text or audio, start with LSTM with one hidden layer or tempooral convs, then go to Attention model or WaveNet-like model)
Other (Start with fully connected NN with one hidden layer)
Initialization: He et al. normal (reul), Glorot normal (tanh)
Regularization: Start with None, then add it in later
Data normalization: Start with None because it adds bugs. Then add in later


3 mains parts of a CNN:
1) convolutions - for extracting the features in the Images
2) apply non linearity: often ReLU
3) Pooling: Downsampling operating in each feature map

RNN (Recurrent Neural Network):
Processing sequential data
Networks with loops in them.

vanishing gradients is a problem (if you multiply a number continuously by something bn 0 and 1, it eventually goes to 0). To put it another way, it remembers more recent information

To get around this, LSTM (Long Short Term Memory) is used
1) Forget
2) Store
3) Update
4) Output

Key Concepts of LSTM:
1) Maintain seperate cell state independent from what is outputted
2) Use gates to control the flow of information
2a) forget gate gets rid of irrelevent information
2b) Store relevant information from current intput
2c) Selectiely update cell state
2d) Output gate returns a filtered version of the cell state
3) Backprop through time with uninterrupted gradient flow

RNN Applications:
Music Generation
Sentiment classification
Machine translation
Environmental Modeling


Recommended Starting Points:
Optimizer: Adam with learning rate 3e-4
Activation: reul (FC and Conv Models), tanh (LSTMs)



Generative Adversarial Networks (GANs)
This is how fake things are created.  
subset of unsurpervised learning
Goal is to take input of training samples from a distribution and learn a model that represent that distribution
one way is density estimation
use to debias a model (e.g given a set of faces, what does your drawn picture look like? all white? all male?)
outlier detection

Latent variable models:
Think of Plato's Repulic, the prisoners in the cave look at the shadows.  The actual objects the prisoners don't see are latent variables


Autoencoders:is a form of compression
Simple gernative model, attempts to encode the input
Encoding: unsurpervised, takes higher dimensial input and makes it lower dimensial
Decoding: unsurpervised, takes lower dimensional input and recreates the original data

GANs (creates the faces):
2 networks, generative, and descriminative, which work against each other.  The generator builds something, and the descriminative determines if it's real or fake.  Over time, they both get better





Competitive Data Science in Coursera




Questions:

best way to interpret clusters? (just lookat centroids, or something else?)
Do you use Silhoette score, elbow method or something else to idenify k?
    Answer - Silhouette score is standard (https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6)

PCA - to identify clusters.  But what does this really tell us?
Other clustering algorithms? Do you use any of them?
agglomerative clustering. 

Once you have clusters for each row, then use that as a class to build random forrest, so you can get explainability.  



Any practical applications for GANs?
Explain the Buffer Size. 
Are embedding layers used independently?
What are checkpoints? How often does a checkpoint get created
How does SGD work? If you look at one example, calculate loss, change the network weights, then the next example can be totally different in the hyper plane.  It seems impossible noisy




Parameters for xgboost: depth and number of estimators are common Hyperparmeters
feature importance and explainability in Random Forrest is big.  

Batch Normalization - isn't it a problem if you normalize each batch with different data

tensorboard help. Can you see live runs?
What does an embedding table mean?
RNN and LSTM are too confusing.  No one can understand unless they are an expert.  How to cope? what the autoML equivalent? How to use transfer learning?



autoPyTorch there is a notion of budget and time.  What does budget mean? (# of epochs)
Decision Tree allows me to run when the class is not one hot encoded.  Performance was different. Why?
Familiar with pdpbox? What do the boxes mean?
Why is the tree so small from dTreeViz
What data prep is needed for trees

How to deal with data for bigger models.  Can you batch with sklearn or xgboost or others?
Incremental Learning is a way to deal with bigger data
When buying a computer, if ram is listed as 2x32, does that mean I could fit a 50gb of data in ram as a dataframe?
Is there an auto EDA
Is there any auto data cleaning (auto normalize?)

https://pycaret.org/ used like an automl

