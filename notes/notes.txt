Courses I said I took but did not:



Regex Notes: https://speakerdeck.com/pycon2017/al-sweigart-yes-its-time-to-learn-regular-expressions
Scaling: https://pub.towardsai.net/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff
Missing Data: https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html
Imbalanced Data: https://www.springboard.com/workshops/ai-machine-learning-career-track/learn#/curriculum/17811/17813
LinkedIn Learning Class: https://www.linkedin.com/learning/python-for-data-science-essential-training-part-2/machine-learning-rocks?u=36492188



Linear Regression Assumptions
Linearity: The relationship between X and the mean of Y is linear.
Homoscedasticity: The variance of residual is the same for any value of X.
Independence: Observations are independent of each other.
Normality: For any fixed value of X, Y is normally distributed.


Classificaiton Descriptions:
https://medium.com/@vijaya.beeravalli/comparison-of-machine-learning-classification-models-for-credit-card-default-data-c3cf805c9a5a


Naive Bayes:
Classificaiton algorithm based on Bayes's Theorem which gives an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a feature in a class is unrelated to the presence of any other feature
Pros: Fast, small amount of data needed
Cons: bad estimator

K Nearest Neighbors:
Gives a classification based on classification of other neighbors 


Decision Tree:
Builds classification model in a tree structure. 
Categorical and numerical
Simplicity to understand and visualize
Quite unstable because a change in data can change the entire decision tree

Benefits:
Nonlinear
Can be graphically represented
Requires less data prep (no one hot encoding)
Good with missing features
you can also use the resutling tree space as features for other models (ie, the leaf space)

Assumptions:
Root node = entire training set
Predictive featuresmust be cateogrical or binned (if continuous)
Rows in the dataset have a recursive distribution based on the values of the attributes

Random Forrest:
Many decision trees, can give average of multiple trees.
More accurate than decision tree
Quite complex in implementation and slow in real time prediction
Bootstrapping is random sampling of data with replacement
Bagging (Bootrap Aggregation) is performing bootstrapping many times and building an estimator for each bootstrapped dataset
Bagging works well (averaging the estimators) because the data is selected randomly.  If there is a correlation in the data, then the average isn't as powerful


ANN:
Poor inte

SVM:

Hyperparmeters to train:
C = penalty term that dertermines how closely the model fits to the training set. Basically a regularization parameter.  Large penalty for misclassifciation in training when C is big. This gives potential for overfit. 
As C goes to 0, it will give a large margin of error. 
Kernel to pick from is also a Hyperparmeter 
classifier that represents training data in poitns in space and then 

Uses a subset of data, which makes it memory efficient and is good with high dimensial space
does not give probability estimates

When to use SVM:
Binary target variable
Feature to row ratio is high (short and wide data)
Very complex relationships
lots of outliers

When not to use SVM:
feature to row ratio is very low (SVM is slow with lots of data)
If transparency is important
Looking for a quick benchmark model.  Slow in training and prediction




Competitive Data Science in Coursera


Questions:
autoPyTorch there is a notion of budget and time.  What does budget mean?
Decision Tree allows me to run when the class is not one hot encoded.  Performance was different. Why?
Familiar with pdpbox? What do the boxes mean?
Why is the tree so small from dTreeViz
What data prep is needed for trees

How to deal with data for bigger models.  Can you batch with sklearn or xgboost or others?
Incremental Learning is a way to deal with bigger data
When buying a computer, if ram is listed as 2x32, does that mean I could fit a 50gb of data in ram as a dataframe?
Is there an auto EDA
Is there any auto data cleaning (auto normalize?)

https://pycaret.org/ used like an automl

